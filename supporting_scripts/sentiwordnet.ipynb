{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from twitter_preprocessor import TwitterPreprocessor\n",
    "import os\n",
    "import glob\n",
    "# from textblob import TextBlob\n",
    "# from textblob import Blobber\n",
    "# from textblob.sentiments import NaiveBayesAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cary/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/cary/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/cary/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cary/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# preperation\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('stopwords')\n",
    "stopset = set(stopwords.words('english'))\n",
    "# blobSenti = Blobber(analyzer = NaiveBayesAnalyzer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where stores the total data\n",
    "soldierCalDict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file list\n",
    "soldierCSVs = []\n",
    "files = os.listdir(os.getcwd() + \"/data/soldiers\")\n",
    "for file in files:\n",
    "     if not os.path.isdir(file) and file.endswith(\".csv\"):\n",
    "            soldierCSVs.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useless columns for analyzing\n",
    "uselessColumns = [\n",
    "    \"id\",\n",
    "    \"conversation_id\",\n",
    "    \"place\",\n",
    "    \"photos\",\n",
    "    \"video\",\n",
    "    \"near\",\n",
    "    \"geo\",\n",
    "    \"source\",\n",
    "    \"user_rt_id\",\n",
    "    \"user_rt\",\n",
    "    \"retweet_id\",\n",
    "    \"reply_to\",\n",
    "    \"retweet_date\",\n",
    "    \"translate\",\n",
    "    \"trans_src\",\n",
    "    \"trans_dest\",\n",
    "    \"link\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pos from nltk.pos_tag to SentiWordNet\n",
    "# n - NOUN \n",
    "# v - VERB \n",
    "# a - ADJECTIVE \n",
    "# s - ADJECTIVE SATELLITE \n",
    "# r - ADVERB\n",
    "# ' ' - others\n",
    "# a list of nltk tags is here:\n",
    "# https://www.techrepublic.com/article/the-6-laws-every-cloud-architect-should-know-according-to-werner-vogels/\n",
    "def sentiWordNetPOSconvetor(pos):\n",
    "    newtag = ''\n",
    "    if pos.startswith('NN'):\n",
    "        newtag='n'\n",
    "    elif pos.startswith('JJ'):\n",
    "        newtag='a'\n",
    "    elif pos.startswith('V'):\n",
    "        newtag='v'\n",
    "    elif pos.startswith('R'):\n",
    "        newtag='r'\n",
    "    return newtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word: single word\n",
    "# pos: part of speech from nltk.pos_tag\n",
    "def calSentiForWord(word, pos):\n",
    "    \n",
    "    sentis = list(swn.senti_synsets(word, sentiWordNetPOSconvetor(pos)))\n",
    "    \n",
    "    if len(sentis) <= 0:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    # Getting average of all possible sentiments\n",
    "    positive = 0.0\n",
    "    negative = 0.0\n",
    "    objective = 0.0\n",
    "    count = 0.0\n",
    "    \n",
    "    for senti in sentis :\n",
    "        positive += senti.pos_score()\n",
    "        negative += senti.neg_score()\n",
    "        objective += senti.obj_score()\n",
    "        count += 1\n",
    "        \n",
    "    if count <= 0.1:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    positive /= count\n",
    "    negative /= count\n",
    "    objective /= count\n",
    "    \n",
    "    return positive, negative, objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calSentiForTweet(tweet):\n",
    "    tempTweet = tweet\n",
    "    tempTweetCleaned = TwitterPreprocessor(tempTweet).fully_preprocess().text\n",
    "    if len(tempTweetCleaned) <= 0:\n",
    "        return 0, 0, 0, 0\n",
    "    # miss spelling\n",
    "#     tempTweetCleaned = str(blobSenti(tempTweetCleaned).correct())\n",
    "    \n",
    "#     tempTokens = TweetTokenizer.tokenize(tempTweetCleaned)\n",
    "    tempTokens = nltk.word_tokenize(tempTweetCleaned)\n",
    "#     tempTokens = [tempToken.lower() for tempToken in tempTokens if len(tempToken)>2]\n",
    "#     tempTokens = [tempToken.lower() for tempToken in tempTokens if tempToken.lower() not in stopset and len(tempToken)>2]\n",
    "    tempTagged = nltk.pos_tag(tempTokens)\n",
    "    \n",
    "    positive = 0.0\n",
    "    negative = 0.0\n",
    "    objective = 0.0\n",
    "    count = 0.0\n",
    "    \n",
    "#     print(tempTagged)\n",
    "    \n",
    "    for w, p in tempTagged:\n",
    "        po, ne, ob = calSentiForWord(w, p)\n",
    "        positive += po\n",
    "        negative += ne\n",
    "        objective += ob\n",
    "        count += 1\n",
    "        \n",
    "    if count <= 0.1:\n",
    "        return 0, 0, 0, 0\n",
    "        \n",
    "    positive /= count\n",
    "    negative /= count\n",
    "    objective /= count\n",
    "    \n",
    "#     tb = blobSenti(tempTweetCleaned)\n",
    "    \n",
    "    return count, positive, negative, objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating... fcharles81.csv\n",
      "0\n",
      "1000\n",
      "2000\n",
      "{'count': 2483, 'tweet cleaned length mean': 6.074103906564639, 'tweet cleaned length std': 4.6438334637779715, 'positive mean': 0.04363063746349662, 'positive std': 0.07174881839965981, 'negative mean': 0.028761599724049472, 'negative std': 0.046999154551966855, 'objective mean': 0.48632191720996093, 'objective std': 0.27967403541227165}\n",
      "Calculating... GeoffMillard.csv\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "{'count': 10766, 'tweet cleaned length mean': 9.522292402006316, 'tweet cleaned length std': 6.618489109211659, 'positive mean': 0.04666032808702625, 'positive std': 0.05312202909293429, 'negative mean': 0.03463506803356697, 'negative std': 0.03901411595826622, 'objective mean': 0.5615717230805968, 'objective std': 0.2312752604300263}\n",
      "Calculating... cNikonphoto.csv\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "{'count': 6981, 'tweet cleaned length mean': 4.879100415413265, 'tweet cleaned length std': 4.6691108372673495, 'positive mean': 0.017179705321228464, 'positive std': 0.03535265326917498, 'negative mean': 0.010115591668261854, 'negative std': 0.02319234229946219, 'objective mean': 0.40290424361483707, 'objective std': 0.33144415979202346}\n",
      "Calculating... veteranhank.csv\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-031598907007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtempDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalSentiForTweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtweetCleanedLengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpositives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-907b1dda70e3>\u001b[0m in \u001b[0;36mcalSentiForTweet\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#     tempTokens = [tempToken.lower() for tempToken in tempTokens if len(tempToken)>2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     tempTokens = [tempToken.lower() for tempToken in tempTokens if tempToken.lower() not in stopset and len(tempToken)>2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtempTagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtempTokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpositive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TextAnalytics-9hFs7Vd6/lib/python3.7/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \"\"\"\n\u001b[1;32m    160\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TextAnalytics-9hFs7Vd6/lib/python3.7/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    116\u001b[0m         )\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Maps to the specified tagset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TextAnalytics-9hFs7Vd6/lib/python3.7/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_conf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TextAnalytics-9hFs7Vd6/lib/python3.7/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, features, return_conf)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for soldierCSVFile in soldierCSVs:\n",
    "    if soldierCSVFile == 'cleaned_vet_tweet_df.csv': continue\n",
    "    print('Calculating... ' + soldierCSVFile)\n",
    "    \n",
    "    tempDF = pd.read_csv(os.getcwd() + \"/data/soldiers/\" + soldierCSVFile, encoding='utf8')\n",
    "    tempDF.dropna(subset=['tweet'], inplace=True) # clear empty tweets\n",
    "    tempDF.drop(uselessColumns, axis=1, inplace=True)\n",
    "    \n",
    "    positives = []\n",
    "    negatives = []\n",
    "    objectives = []\n",
    "#     polarities = []\n",
    "#     subjectivities = []\n",
    "    tweetCleanedLengths = []\n",
    "    \n",
    "    for id, row in tempDF.iterrows():\n",
    "        tokens, po, ne, ob = calSentiForTweet(row['tweet'])\n",
    "        tweetCleanedLengths.append(tokens)\n",
    "        positives.append(po)\n",
    "        negatives.append(ne)\n",
    "        objectives.append(ob)\n",
    "#         polarities.append(pola)\n",
    "#         subjectivities.append(sub)\n",
    "        \n",
    "        if (id % 1000) == 0: print(id)\n",
    "#         if (soldierCSVFile == 'cNikonphoto.csv'): print(id)\n",
    "    \n",
    "    count = len(tweetCleanedLengths)\n",
    "    \n",
    "    # Computing the standard deviation in float64 is more accurate:\n",
    "    # np.std([1, 2, 3, 4], dtype=np.float64)\n",
    "    if count > 0:\n",
    "        result = {\n",
    "            'count': count,\n",
    "            'tweet cleaned length mean': np.mean(tweetCleanedLengths),\n",
    "            'tweet cleaned length std': np.std(tweetCleanedLengths, dtype=np.float64),\n",
    "            'positive mean': np.mean(positives),\n",
    "            'positive std': np.std(positives, dtype=np.float64),\n",
    "            'negative mean': np.mean(negatives),\n",
    "            'negative std': np.std(negatives, dtype=np.float64),\n",
    "            'objective mean': np.mean(objectives),\n",
    "            'objective std': np.std(objectives, dtype=np.float64),\n",
    "#             'polarity mean': np.mean(polarities),\n",
    "#             'polarity std': np.std(polarities, dtype=np.float64),\n",
    "#             'subjectivity mean': np.mean(subjectivities),\n",
    "#             'subjectivity std': np.std(subjectivities, dtype=np.float64),\n",
    "        }\n",
    "    else:\n",
    "        result = {\n",
    "            'count': 0,\n",
    "            'tweet cleaned length mean': 0,\n",
    "            'tweet cleaned length std': 0,\n",
    "            'positive mean': 0,\n",
    "            'positive std': 0,\n",
    "            'negative mean': 0,\n",
    "            'negative std': 0,\n",
    "            'objective mean': 0,\n",
    "            'objective std': 0,\n",
    "#             'polarity mean': 0,\n",
    "#             'polarity std': 0,\n",
    "#             'subjectivity mean': 0,\n",
    "#             'subjectivity std': 0\n",
    "        }\n",
    "    \n",
    "    soldierCalDict[soldierCSVFile] = result\n",
    "    \n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame.from_dict(soldierCalDict, orient=\"index\").to_csv(\"soldierCal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
